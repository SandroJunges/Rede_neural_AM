{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Redes Neurais | Trabalho Final da disciplina de Aprendizado de Máquina INE5664",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Implementação de uma Rede Neural Multitarefa\n\n## Introdução\nEste notebook apresenta a implementação de uma rede neural customizada capaz de lidar com tarefas de:\n1. **Classificação Binária**\n2. **Regressão**\n3. **Classificação Multiclasse**\n\nEle inclui funções de ativação, funções de perda, retropropagação e um modelo de rede neural treinável, com dados simulados ou reais.\n\n---\n\n## Importação de Bibliotecas Necessárias\nAs bibliotecas são fundamentais para manipulação de dados, cálculos e avaliação do modelo.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings('ignore') ## Desativa os avisos exibidos durante a execução\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score ## Define as Métricas de Avaliação\nfrom sklearn.model_selection import train_test_split ## Divide o conjunto de dados em conjuntos de treino e teste\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler ## Normaliza os dados para garantir que tenham média 0 e desvio padrão 1\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "1. **Warnings**: usada para controlar alertas gerados pelo Python durante a execução do código.\n2. **Pandas**: usada para manipulação e análise de dados. Amplamente utilizada em projetos de aprendizado de máquina devido à sua capacidade de trabalhar com grandes conjuntos de dados de forma eficiente.\n3. **NumPy**:  base fundamental para cálculos numéricos e operações matemáticas em Python.\n4. **Scikit-learn**:  fornece ferramentas simples e eficientes para aprendizado de máquina.\n\n---\n\n## Métricas de avaliação do projeto\n\n- **mean_squared_error**: Mede o erro médio ao quadrado entre os valores reais e previstos (usado em regressão).\n- **r2_score**: Mede o quão bem os dados se ajustam ao modelo (valores próximos de 1 indicam bom ajuste).\n- **accuracy_score**: Mede a porcentagem de previsões corretas (usado em classificação).\n\n---\n\n## Definição das Funções de Ativação\n\nFunções de ativação determinam a saída de um neurônio com base nos dados de entrada e nos pesos aplicados. Em outras palavras, as funções de ativação introduzem não-linearidade no modelo, permitindo que ele aprenda e represente relações complexas entre os dados de entrada e saída.\n\n### ReLU",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Funções de Ativação\ndef relu(z):\n    return np.maximum(0, z)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "- **Descrição**: A função ReLU retorna o valor de entrada diretamente se for positivo; caso contrário, retorna zero.\n\n- **Propósito**: Introduz não-linearidade mantendo simplicidade computacional. É amplamente usada em camadas ocultas de redes neurais profundas.\n\n---\n\n### Sigmoid",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def sigmoid(z):\n    return 1 / (1 + np.exp(-z))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "- **Descrição**: A função Sigmoid transforma qualquer valor real em um intervalo entre 0 e 1, funcionando como uma probabilidade.\n\n- **Propósito**: Ideal para problemas de classificação binária, onde a saída deve ser interpretada como uma probabilidade.\n\n---\n\n### Softmax",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def softmax(z):\n    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "- **Descrição**: A função Softmax converte um vetor de valores em uma distribuição de probabilidades. Cada saída é normalizada para somar 1.\n\n- **Propósito**: Usada em problemas de classificação multiclasse para calcular a probabilidade de cada classe.\n\n---\n\n### Linear",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def linear(z):\n    return z",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "- **Descrição**: A função Linear retorna a entrada sem modificações, ou seja, f(z)=z.\n\n- **Propósito**: Usada em problemas de classificação multiclasse para calcular a probabilidade de cada classe.\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Derivadas das Funções de Ativação\n\nAs derivadas são essenciais para o cálculo de gradientes na retropropagação.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Derivadas das Funções de Ativação\ndef relu_derivative(z):\n    return (z > 0).astype(float)\n\n# Dicionários de funções para fácil acesso\nactivation_functions = {\n    'relu': relu,\n    'sigmoid': sigmoid,\n    'softmax': softmax,\n    'linear': linear\n}\n\nactivation_derivatives = {\n    'relu': relu_derivative\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n## Funções de Perda\n\nAs funções de perda medem a discrepância entre a saída do modelo e os valores reais.\n\n- **Binary Loss**: Usada para classificação binária.\n- **Regression Loss**: Usada para regressão.\n- **Multiclass Loss**: Usada para classificação multiclasse.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Funções de Perda\ndef binary_loss(y, output):\n    epsilon = 1e-15\n    output = np.clip(output, epsilon, 1 - epsilon)\n    return -np.mean(y * np.log(output) + (1 - y) * np.log(1 - output))\n\ndef regression_loss(y, output):\n    return np.mean((output - y) ** 2)\n\ndef multiclass_loss(y, output):\n    epsilon = 1e-15\n    output = np.clip(output, epsilon, 1 - epsilon)\n    y = y.astype(int)\n    log_probs = -np.log(output[np.arange(y.shape[0]), y.flatten()])\n    return np.mean(log_probs)\n\n# Dicionário de funções de perda\nloss_functions = {\n    'binary': binary_loss,\n    'regression': regression_loss,\n    'multiclass': multiclass_loss\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### y\n\n- **Ground Truth (Valor Real)**\n- É o conjunto de valores reais (ou verdadeiros) associados a cada exemplo do conjunto de dados. Representa o que esperamos que o modelo preveja.\n- Estes valores vêm do dataset usado para treinamento, validação ou teste. Geralmente são os rótulos de classe para problemas de classificação ou valores contínuos para regressão.\n\n### output\n\n- **Previsões do Modelo**\n- É o conjunto de valores previstos pelo modelo em resposta a uma entrada específica.\n- Esses valores são gerados pelo passo forward da rede neural, aplicando funções de ativação na saída dos neurônios.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n## Implementação da Classe NeuralNetwork\n\nA classe `NeuralNetwork` encapsula:\n- Inicialização dos pesos.\n- Forward propagation.\n- Backward propagation (retropropagação).\n- Treinamento.\n- Avaliação.\n\n### def __init__\n\nEsse método inicializa os atributos da rede neural e define os **parâmetros** necessários para configurá-la:\n\n- **input_size**: Número de neurônios na camada de entrada, ou seja, o número de características do conjunto de dados (dimensão dos dados de entrada).\n\n- **hidden_neurons**: Número de neurônios na camada oculta. Este número afeta a capacidade da rede de capturar padrões nos dados.\n\n- **output_neurons**: Número de neurônios na camada de saída. Geralmente corresponde ao número de classes (classificação) ou à dimensão da saída (regressão).\n\n- **activation**: Nome da função de ativação ('relu', 'sigmoid', etc.). Esta função é usada para calcular a saída dos neurônios na camada oculta.\n\n- **loss**: Nome da função de perda ('binary', 'regression', 'multiclass'). Define como a rede avaliará a diferença entre as previsões e os valores reais.\n\n- **learning_rate**: Taxa de aprendizado usada para atualizar os pesos durante a otimização.\n\n- **task_type**: Tipo de tarefa que a rede executará ('binary', 'regression' ou 'multiclass'). Determina o comportamento da função forward e da função de perda.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class NeuralNetwork:\n    def __init__(self, input_size, hidden_neurons, output_neurons, activation, loss, learning_rate, task_type):\n        self.input_size = input_size\n        self.hidden_neurons = hidden_neurons\n        self.output_neurons = output_neurons\n        self.activation = activation_functions[activation]\n        self.activation_derivative = activation_derivatives.get(activation)\n        self.loss = loss_functions[loss]\n        self.learning_rate = learning_rate\n        self.task_type = task_type",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#### Inicialização dos pesos\n\nO trecho de inicialização dos pesos no método __init__ da classe NeuralNetwork é essencial para garantir que a rede neural comece o treinamento com valores adequados para uma boa convergência.\n\n1. **Pesos (W)**:\n\n- São os parâmetros ajustáveis da rede que conectam os neurônios entre as camadas.\n- Determinam a força e a direção da influência de uma entrada ou de uma camada sobre a próxima.\n\n2. **Bias (B)**:\n\n- São valores adicionados ao somatório ponderado das entradas antes de passar pela função de ativação.\n- Permitem à rede neural se ajustar melhor ao deslocamento dos dados e evitar ser limitada a passar pela origem.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Inicialização dos pesos\nself.W1 = np.random.randn(input_size, hidden_neurons) * np.sqrt(2 / input_size)\nself.B1 = np.zeros((1, hidden_neurons))\nself.W2 = np.random.randn(hidden_neurons, output_neurons) * np.sqrt(2 / hidden_neurons)\nself.B2 = np.zeros((1, output_neurons))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "\n---\n\n### Método forward\n\nO método forward da classe NeuralNetwork implementa a propagação para frente (forward propagation), que é o processo de calcular a saída da rede neural para uma determinada entrada. Ele usa as entradas, os pesos e os bias para calcular as ativações de cada camada até chegar à saída final. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": " def forward(self, X):\n        # X: Dados de entrada (matriz de dimensão [n_amostras, n_características])\n        # W1: Pesos conectando a entrada à camada oculta\n        # B1: Bias da camada oculta (vetor de dimensão [1, n_neurônios_ocultos])\n        self.z1 = X.dot(self.W1) + self.B1\n\n        # Aplica a função de ativação escolhida na camada oculta\n        # activation: Função como ReLU, sigmoid, etc.\n        self.f1 = self.activation(self.z1)\n\n         # Calcula a entrada para a camada de saída (pré-ativação)\n        # f1: Saída da camada oculta (dimensão [n_amostras, n_neurônios_ocultos])\n        # W2: Pesos conectando a camada oculta à camada de saída\n        # B2: Bias da camada de saída (dimensão [1, n_neurônios_saida])\n        self.z2 = self.f1.dot(self.W2) + self.B2\n\n        # Decide como processar a saída da rede com base no tipo de tarefa\n        if self.task_type == 'binary':\n            return sigmoid(self.z2)\n        elif self.task_type == 'regression':\n            return self.z2\n        elif self.task_type == 'multiclass':\n            return softmax(self.z2)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "\n---\n\n### Método Backward\n\nEle realiza a propagação para trás (backpropagation), o processo usado para calcular os gradientes da função de perda em relação aos pesos da rede, permitindo ajustar os pesos durante o treinamento.\n\n#### Erros na camada de saída (delta2)\n\nDepende do tipo de tarefa:\n\n- **Binária**: diferença entre a saída prevista (output) e o rótulo real (y)\n- **Regressão**: diferença contínua entre a saída e os rótulos\n- **Multiclasse**: subtrai 1 da probabilidade da classe verdadeira",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def backward(self, X, y, output):\n    # Número de exemplos no lote de entrada\n    m = X.shape[0] \n\n    # Calcula o erro na camada de saída (delta2) com base no tipo de tarefa\n    if self.task_type == 'binary':\n        delta2 = output - y\n    elif self.task_type == 'regression':\n        y = y.reshape(-1, self.output_neurons)  # Garante que `y` tenha a forma esperada\n        delta2 = output - y\n    elif self.task_type == 'multiclass':\n        delta2 = output  # Inicialmente, `delta2` é a saída\n        delta2[np.arange(m), y.flatten()] -= 1  # Ajusta para refletir o erro da classe correta",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "\nNo método backward, após calcular a saída da rede e o erro em relação aos valores reais, **os gradientes são calculados para ajustar os parâmetros da rede neural (pesos e biases)**.\n\nEsses gradientes indicam a direção e a magnitude da modificação necessária para minimizar a função de perda.\n\n1. **dW2**\n\n- Gradiente da perda em relação aos pesos da camada de saída.\n- Representa como os pesos devem ser ajustados para reduzir o erro.\n\n2. **dB2**\n\n- Gradiente da perda em relação aos biases da camada de saída.\n- Representa como os biases devem ser ajustados para reduzir o erro.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "    dW2 = self.f1.T.dot(delta2) / m  # Gradiente do peso: derivada de perda em relação a W2\n    dB2 = np.sum(delta2, axis=0, keepdims=True) / m  # Gradiente do bias: média do erro na saída",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Agora, o trecho seguinte do código faz parte do cálculo do gradiente da camada oculta durante a etapa de retropropagação (backpropagation). Ele **calcula como o erro na camada de saída influencia a camada oculta**, permitindo que os pesos e biases dessa camada também sejam ajustados para reduzir a perda.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "    # Calcula o erro propagado para a camada oculta (delta1)\n    if self.activation_derivative:\n        # Propaga o erro da camada de saída para a camada oculta\n        # Ajusta o erro considerando a função de ativação\n        delta1 = delta2.dot(self.W2.T) * self.activation_derivative(self.z1)\n    else:\n        # Se a função de ativação não tiver derivada definida, apenas propaga o erro\n        delta1 = delta2.dot(self.W2.T)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Sendo assim, agora são definidos os gradiente da camada oculta.\n\n1. **dW1**: Gradiente dos pesos da camada oculta; calcula como cada peso contribui para o erro, considerando a entrada X.\n\n2. **dB1**: Gradiente dos biases da camada oculta; reflete o erro acumulado de cada neurônio.\n\nEsses gradientes são a base para ajustar os parâmetros do modelo, reduzindo a função de perda e melhorando a performance.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "  # Gradiente dos pesos e bias da camada oculta (W1 e B1)\n    dW1 = X.T.dot(delta1) / m  # Gradiente do peso: derivada de perda em relação a W1\n    dB1 = np.sum(delta1, axis=0, keepdims=True) / m  # Gradiente do bias: média do erro na camada oculta",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Na última parte desse método, é realizada a **atualização dos pesos e biases do modelo após o cálculo dos gradientes**, com o objetivo de minimizar o erro (perda) durante o treinamento.\n\n- Cada parâmetro (peso W e bias B) é ajustado subtraindo o gradiente multiplicado pela taxa de aprendizado (learning_rate).\n- Isso move os parâmetros na direção que diminui o erro.\n- A atualização busca reduzir a perda, ajustando os parâmetros para melhorar a predição do modelo, baseado nos gradientes calculados durante a retropropagação.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "    # Atualização dos pesos\n    self.W1 -= self.learning_rate * dW1\n    self.B1 -= self.learning_rate * dB1\n    self.W2 -= self.learning_rate * dW2\n    self.B2 -= self.learning_rate * dB2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### Método train\n\nA função train é responsável por treinar o modelo de rede neural, iterando sobre os dados de treinamento por um número específico de épocas. Ela realiza a propagação para frente (forward), calcula a perda (loss) e faz a retropropagação (backward) para atualizar os pesos e biases.\n\n1. **Loop de épocas**\n\n- O treinamento ocorre por várias épocas (iterações), definidas pelo parâmetro epochs.\n- Em cada época, o modelo passa pelos seguintes passos:\n\n2. **Propagação para frente (Forward)**\n\n- A função `self.forward(X_train)` é chamada para calcular a saída do modelo com os dados de treinamento.\n- A saída é comparada com a saída real (y_train), e a função de perda calcula o erro.\n\n3. **Cálculo da perda (Loss)**\n\n- A perda é calculada com a função `self.loss(y_train, output)`, que mede a diferença entre a previsão do modelo e o valor real.\n- A função de perda depende do tipo de tarefa (classificação binária, regressão ou classificação multiclasse).\n\n4. **Retropropagação (Backward)**\n\n- A função `self.backward(X_train, y_train, output)` é chamada para calcular os gradientes da perda em relação aos pesos e biases.\n- Esses gradientes são usados para ajustar os parâmetros do modelo (pesos e biases).\n\n5. **Validação**\n\n- O modelo é avaliado usando o conjunto de dados de validação (X_val, y_val) para verificar se está generalizando bem.\n- A perda no conjunto de validação é calculada e armazenada em val_loss.\n\n6. **Exibição do progresso**\n\n- Se verbose for True e a época for múltiplo de 300, o código exibe a perda de treinamento e de validação para monitorar o progresso do treinamento.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def train(self, X_train, y_train, X_val, y_val, epochs, verbose=True):\n    for epoch in range(epochs):\n        # Passo 1: Propagação para frente (Forward)\n        output = self.forward(X_train)\n        loss = self.loss(y_train, output)  # Calcula a perda (erro)\n\n        # Passo 2: Retropropagação (Backward)\n        self.backward(X_train, y_train, output)  # Ajusta os pesos com base no erro\n\n        # Passo 3: Avaliação do modelo com dados de validação\n        val_output = self.forward(X_val)\n        val_loss = self.loss(y_val, val_output)  # Perda no conjunto de validação\n\n        # Exibe informações a cada 300 épocas (caso verbose seja True)\n        if verbose and epoch % 300 == 0:\n            print(f\"Época {epoch+1}/{epochs}, Perda Treino: {loss:.4f}, Perda Validação: {val_loss:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### Método evaluate\n\nA função evaluate é responsável por avaliar o desempenho do modelo após o treinamento, utilizando os dados de teste.\n\n1. **Calcula as previsões:** A função chama self.forward(X_test) para obter as previsões do modelo usando os dados de teste (X_test).\n\n2. **Aí, dependendo da tarefa**:\n\n- **Classificação Binária:** Se for uma tarefa de classificação binária, a previsão é convertida para 0 ou 1 (usando um limiar de 0.5). A acurácia é então calculada e exibida.\n\n- **Regressão:** Para regressão, as previsões são comparadas com os valores reais (de teste). São calculados o erro médio quadrático (MSE) e o R², que medem a qualidade da previsão.\n\n- **Classificação Multiclasse:** Para a classificação multiclasse, a previsão é transformada em um índice de classe (com argmax), e a acurácia é calculada.\n\n3. **Exibe o resultado**: Dependendo da tarefa, a função imprime a acurácia ou as métricas de regressão (MSE e R²).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def evaluate(self, X_test, y_test, y_mean=None, y_std=None):\n    # Passo 1: Obter previsões do modelo para os dados de teste\n    predictions = self.forward(X_test)\n\n    # Se a tarefa for de classificação binária:\n    if self.task_type == 'binary':\n        # Converter as previsões para 0 ou 1 com base no limiar de 0.5\n        predictions = (predictions > 0.5).astype(int)\n        # Calcular a acurácia comparando as previsões com os valores reais\n        accuracy = accuracy_score(y_test, predictions)\n        # Imprimir a acurácia e retornar o valor\n        print(f\"Acurácia: {accuracy:.4f}\")\n        return accuracy\n\n    # Se a tarefa for de regressão:\n    if self.task_type == 'regression':\n        # Se os valores de y foram normalizados, denormalizar as previsões e os valores reais\n        predictions = predictions * y_std + y_mean\n        y_test = y_test * y_std + y_mean\n        # Calcular o MSE e o R² para avaliar o desempenho do modelo\n        mse = mean_squared_error(y_test, predictions)\n        r2 = r2_score(y_test, predictions)\n        # Imprimir o MSE e o R² e retornar os valores\n        print(f\"MSE: {mse:.4f}, R² Score: {r2:.4f}\")\n        return mse, r2\n\n    # Se a tarefa for de classificação multiclasse:\n    elif self.task_type == 'multiclass':\n        # Para cada previsão, pegar o índice da classe com a maior probabilidade\n        predictions = np.argmax(predictions, axis=1)\n        # Calcular a acurácia para comparar as previsões com os valores reais\n        accuracy = accuracy_score(y_test, predictions)\n        # Imprimir a acurácia e retornar o valor\n        print(f\"Acurácia: {accuracy:.4f}\")\n        return accuracy\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### Método load_data\n\nA função load_data é responsável por **carregar e pré-processar os dados a partir de um arquivo CSV**.\n\nEla prepara os dados para serem usados no treinamento e avaliação de um modelo de aprendizado de máquina. A função adapta o processo de carregamento conforme o tipo da tarefa (task_type), que pode ser classificação binária, regressão, ou classificação multiclasse.\n\n1. **Carregar os dados**: A função começa carregando os dados de um arquivo CSV utilizando pd.read_csv(file_path).\n\n2. **Tarefa binária (task_type == 'binary')**:\n\n- As variáveis independentes (entradas) e dependentes (saídas) são separadas.\n- A variável de saída (y) é transformada para uma matriz coluna.\n- Os dados são divididos em treino e teste (80% treino, 20% teste).\n- As variáveis de entrada são normalizadas com StandardScaler.\n- Retorna os dados de treino e teste.\n\n3. **Tarefa de regressão (task_type == 'regression')**:\n\n- Seleciona as variáveis de entrada e saída (por exemplo, TV, Rádio, Jornal e Vendas).\n- As variáveis de entrada e saída são normalizadas separadamente.\n- Divide os dados em treino e teste (80% treino, 20% teste).\n- Retorna os dados de treino e teste com as médias e desvios padrão para reverter a normalização.\n\n4. **Tarefa multiclasse (task_type == 'multiclass')**:\n\n- A coluna de classes é codificada numericamente.\n- As variáveis de entrada são normalizadas.\n- Divide os dados em treino e teste (80% treino, 20% teste), mantendo a proporção das classes.\n- Retorna os dados de treino e teste com a codificação das classes.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Função para carregar os dados\ndef load_data(file_path, task_type):\n    data = pd.read_csv(file_path)  # Carrega os dados do CSV\n    if task_type == 'binary':  # Se a tarefa for binária\n        X = data.iloc[:, :-1].values  # Extrai as variáveis independentes\n        y = data.iloc[:, -1].values  # Extrai a variável dependente\n\n        y = y.reshape(-1, 1)  # Ajusta a forma de y\n\n        # Divide os dados em treino e teste (80/20)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        # Normaliza os dados de entrada\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n        return X_train, X_test, y_train, y_test, None, None  # Retorna os dados normalizados\n\n    elif task_type == 'regression':  # Se a tarefa for de regressão\n        dados_multi = data[['TV', 'Radio', 'Newspaper', 'Sales']]  # Seleciona as variáveis relevantes\n        X_multi = dados_multi[['TV', 'Radio', 'Newspaper']].values  # Variáveis independentes\n        y_multi = dados_multi['Sales'].values  # Variável dependente\n        \n        # Normaliza as variáveis de entrada (X) e saída (y)\n        scaler_X = StandardScaler()\n        X_multi_scaled = scaler_X.fit_transform(X_multi)\n        scaler_y = StandardScaler()\n        y_multi_scaled = scaler_y.fit_transform(y_multi.reshape(-1, 1)).flatten()\n\n        # Divide os dados em treino e teste\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_multi_scaled, y_multi_scaled, test_size=0.2, random_state=0\n        )\n        return X_train, X_test, y_train, y_test, scaler_y.mean_[0], scaler_y.scale_[0]  # Retorna os dados e parâmetros de normalização\n\n    elif task_type == 'multiclass':  # Se a tarefa for multiclasse\n        target = data.pop('Species')  # Extrai a coluna de classes (ajustar conforme necessário)\n        class_map = {label: idx for idx, label in enumerate(target.unique())}  # Mapeia as classes para números\n        target_encoded = target.map(class_map).values  # Codifica as classes\n\n        # Normaliza as variáveis de entrada\n        scaler = StandardScaler()\n        dados_normalizados = scaler.fit_transform(data)\n\n        # Divide os dados em treino e teste\n        X_train, X_test, y_train, y_test = train_test_split(\n            dados_normalizados,\n            target_encoded,\n            test_size=0.2,\n            random_state=0,\n            stratify=target_encoded  # Mantém a proporção das classes\n        )\n        return X_train, X_test, y_train, y_test, None, None  # Retorna os dados e as classes codificadas\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### Método train_and_evaluate\n\nA função train_and_evaluate é responsável por treinar e avaliar um modelo de rede neural, passando por várias etapas, como carregar dados, configurar o modelo, treinar e avaliar sua performance.\n\n1. **Exibição do tipo de tarefa**: Exibe uma mensagem indicando qual tipo de tarefa (classificação binária, regressão ou classificação multiclasse) o modelo vai realizar.\n\n2. **Carregar os dados**\n\n- Chama a função load_data que carrega os dados de um arquivo CSV especificado pelo dataset_path. Ela retorna:\n- X_train e X_test: dados de entrada (variáveis independentes) para treino e teste.\n- y_train e y_test: os rótulos (variáveis dependentes) para treino e teste.\n- y_mean e y_std: valores de média e desvio padrão para normalização, caso a tarefa seja de regressão.\n\n3. **Configuração do modelo**: O modelo é configurado com os parâmetros.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def train_and_evaluate(dataset_path, task_type, activation, loss, output_neurons, hidden_neurons, learning_rate, epochs=1500):\n    print(f\"\\n=== Treinando modelo para tarefa: {task_type} ===\")\n    \n    # Carregar os dados\n    X_train, X_test, y_train, y_test, y_mean, y_std = load_data(dataset_path, task_type)\n    \n    # Configurar o modelo\n    model = NeuralNetwork(\n        input_size=X_train.shape[1],\n        hidden_neurons=hidden_neurons,\n        output_neurons=output_neurons,\n        activation=activation,\n        loss=loss,\n        learning_rate=learning_rate,\n        task_type=task_type\n    )",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "A parte seguinte do código é responsável por treinar e avaliar o modelo de rede neural.\n\n1. **Treinamento do modelo**\n\n- `model.train(...)`: Chama o método train da classe NeuralNetwork, passando os dados de treino (X_train e y_train) e os dados de teste (X_test e y_test).\n- `epochs=epochs`: Define o número de épocas (iterações) para o treinamento do modelo. O valor de epochs é passado como argumento na chamada da função train_and_evaluate, e determina quantas vezes o modelo será treinado com os dados de entrada.\n\n2. **Avaliação do modelo**\n\n- `model.evaluate(...)`: Chama o método evaluate da classe NeuralNetwork para avaliar o desempenho do modelo usando os dados de teste (X_test e y_test).\n- y_mean e y_std: São passados para o método evaluate caso a tarefa seja de regressão, para permitir que o modelo denormalize os valores preditos.\n- A função evaluate retorna o desempenho do modelo, como precisão (para tarefas de classificação) ou erro (para regressão), e esses resultados são armazenados na variável results.\n\n3. **Retorna os dados**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "    # Treinar o modelo\n    model.train(X_train, y_train, X_test, y_test, epochs=epochs)\n    \n    # Avaliar o modelo\n    results = model.evaluate(X_test, y_test, y_mean=y_mean, y_std=y_std)\n    return results",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "O próximo e último bloco de código está dentro de uma estrutura condicional `if __name__ == \"__main__\"`:, o que significa que ele será executado apenas quando o script for executado diretamente (não importado como módulo).\n\nA função executa o treinamento e avaliação de três modelos diferentes, cada um com um tipo de tarefa distinto.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "if __name__ == \"__main__\":  # Verifica se o script está sendo executado diretamente\n    # Treinar e avaliar os 3 modelos\n    \n    # 1. Classificação Binária\n    train_and_evaluate(\n        dataset_path=\"heart binary.csv\",  # Caminho para o arquivo de dados de classificação binária\n        task_type='binary',  # Define que a tarefa é de classificação binária\n        activation='sigmoid',  # Função de ativação sigmoid, apropriada para tarefas binárias\n        loss='binary',  # Função de perda para classificação binária\n        output_neurons=1,  # Número de neurônios na camada de saída (apenas 1, pois é binário)\n        hidden_neurons=200,  # Número de neurônios na camada oculta\n        learning_rate=0.005  # Taxa de aprendizado\n    )\n\n    # 2. Regressão\n    train_and_evaluate(\n        dataset_path=\"advertisement.csv\",  # Caminho para o arquivo de dados de regressão\n        task_type='regression',  # Define que a tarefa é de regressão (valor contínuo)\n        activation='linear',  # Função de ativação linear, comum para regressão\n        loss='regression',  # Função de perda para regressão\n        output_neurons=1,  # Número de neurônios na camada de saída (1 valor contínuo)\n        hidden_neurons=7,  # Número de neurônios na camada oculta\n        learning_rate=0.005  # Taxa de aprendizado\n    )\n\n    # 3. Classificação Multiclasse\n    train_and_evaluate(\n        dataset_path=\"iris.csv\",  # Caminho para o arquivo de dados de classificação multiclasse\n        task_type='multiclass',  # Define que a tarefa é de classificação multiclasse\n        activation='relu',  # Função de ativação ReLU, comum em redes neurais profundas\n        loss='multiclass',  # Função de perda para classificação multiclasse\n        output_neurons=3,  # Número de neurônios na camada de saída (3 classes possíveis)\n        hidden_neurons=5,  # Número de neurônios na camada oculta\n        learning_rate=0.015  # Taxa de aprendizado\n    )\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}